tensor([1.], device='mps:0')

Training AlexNet version: AlexNet
[1,   200] loss: 2.300
[1,   400] loss: 2.302
[1,   600] loss: 2.299
[1,   800] loss: 2.302
[1,  1000] loss: 2.297
[1,  1200] loss: 2.297
[1,  1400] loss: 2.297
[1,  1600] loss: 2.294
[1,  1800] loss: 2.295
[1,  2000] loss: 2.295
[1,  2200] loss: 2.291
[1,  2400] loss: 2.290
[1,  2600] loss: 2.288
[1,  2800] loss: 2.286
[1,  3000] loss: 2.283
[1,  3200] loss: 2.283
[1,  3400] loss: 2.272
[1,  3600] loss: 2.270
[1,  3800] loss: 2.264
[1,  4000] loss: 2.258
[1,  4200] loss: 2.247
[1,  4400] loss: 2.242
[1,  4600] loss: 2.244
[1,  4800] loss: 2.248
[1,  5000] loss: 2.233
Epoch [1/20], Train Loss: 0.2849, Val Loss: 0.2912, Test Loss: 2.3302Training Acc: 0.1475, Val Acc: 0.0872, Test Acc: 0.0863, 
[2,   200] loss: 2.237
[2,   400] loss: 2.236
[2,   600] loss: 2.222
[2,   800] loss: 2.216
[2,  1000] loss: 2.213
[2,  1200] loss: 2.217
[2,  1400] loss: 2.200
[2,  1600] loss: 2.201
[2,  1800] loss: 2.196
[2,  2000] loss: 2.197
[2,  2200] loss: 2.201
[2,  2400] loss: 2.198
[2,  2600] loss: 2.200
[2,  2800] loss: 2.181
[2,  3000] loss: 2.187
[2,  3200] loss: 2.189
[2,  3400] loss: 2.189
[2,  3600] loss: 2.182
[2,  3800] loss: 2.180
[2,  4000] loss: 2.172
[2,  4200] loss: 2.187
[2,  4400] loss: 2.175
[2,  4600] loss: 2.181
[2,  4800] loss: 2.175
[2,  5000] loss: 2.180
Epoch [2/20], Train Loss: 0.2746, Val Loss: 0.2904, Test Loss: 2.3272Training Acc: 0.2588, Val Acc: 0.1079, Test Acc: 0.1031, 
[3,   200] loss: 2.168
[3,   400] loss: 2.173
[3,   600] loss: 2.169
[3,   800] loss: 2.142
[3,  1000] loss: 2.157
[3,  1200] loss: 2.174
[3,  1400] loss: 2.153
[3,  1600] loss: 2.146
[3,  1800] loss: 2.163
[3,  2000] loss: 2.153
[3,  2200] loss: 2.165
[3,  2400] loss: 2.134
[3,  2600] loss: 2.152
[3,  2800] loss: 2.126
[3,  3000] loss: 2.138
[3,  3200] loss: 2.126
[3,  3400] loss: 2.119
[3,  3600] loss: 2.137
[3,  3800] loss: 2.126
[3,  4000] loss: 2.128
[3,  4200] loss: 2.122
[3,  4400] loss: 2.119
[3,  4600] loss: 2.105
[3,  4800] loss: 2.129
[3,  5000] loss: 2.091
Epoch [3/20], Train Loss: 0.2676, Val Loss: 0.2918, Test Loss: 2.3353Training Acc: 0.3187, Val Acc: 0.1039, Test Acc: 0.1027, 
[4,   200] loss: 2.098
[4,   400] loss: 2.112
[4,   600] loss: 2.118
[4,   800] loss: 2.114
[4,  1000] loss: 2.104
[4,  1200] loss: 2.099
[4,  1400] loss: 2.104
[4,  1600] loss: 2.103
[4,  1800] loss: 2.104
[4,  2000] loss: 2.092
[4,  2200] loss: 2.098
[4,  2400] loss: 2.088
[4,  2600] loss: 2.088
[4,  2800] loss: 2.102
[4,  3000] loss: 2.077
[4,  3200] loss: 2.081
[4,  3400] loss: 2.086
[4,  3600] loss: 2.097
[4,  3800] loss: 2.088
[4,  4000] loss: 2.072
[4,  4200] loss: 2.092
[4,  4400] loss: 2.067
[4,  4600] loss: 2.068
[4,  4800] loss: 2.081
[4,  5000] loss: 2.062
Epoch [4/20], Train Loss: 0.2615, Val Loss: 0.2992, Test Loss: 2.3962Training Acc: 0.3669, Val Acc: 0.0467, Test Acc: 0.0437, 
[5,   200] loss: 2.065
[5,   400] loss: 2.065
[5,   600] loss: 2.057
[5,   800] loss: 2.085
[5,  1000] loss: 2.061
[5,  1200] loss: 2.060
[5,  1400] loss: 2.064
[5,  1600] loss: 2.076
[5,  1800] loss: 2.054
[5,  2000] loss: 2.057
[5,  2200] loss: 2.060
[5,  2400] loss: 2.068
[5,  2600] loss: 2.060
[5,  2800] loss: 2.077
[5,  3000] loss: 2.062
[5,  3200] loss: 2.071
[5,  3400] loss: 2.057
[5,  3600] loss: 2.063
[5,  3800] loss: 2.068
[5,  4000] loss: 2.057
[5,  4200] loss: 2.054
[5,  4400] loss: 2.065
[5,  4600] loss: 2.063
[5,  4800] loss: 2.044
[5,  5000] loss: 2.063
Epoch [5/20], Train Loss: 0.2579, Val Loss: 0.2916, Test Loss: 2.3316Training Acc: 0.3959, Val Acc: 0.1097, Test Acc: 0.1116, 
[6,   200] loss: 2.028
[6,   400] loss: 2.052
[6,   600] loss: 2.057
[6,   800] loss: 2.055
[6,  1000] loss: 2.037
[6,  1200] loss: 2.053
[6,  1400] loss: 2.036
[6,  1600] loss: 2.038
[6,  1800] loss: 2.046
[6,  2000] loss: 2.040
[6,  2200] loss: 2.050
[6,  2400] loss: 2.040
[6,  2600] loss: 2.039
[6,  2800] loss: 2.064
[6,  3000] loss: 2.042
[6,  3200] loss: 2.024
[6,  3400] loss: 2.028
[6,  3600] loss: 2.036
[6,  3800] loss: 2.027
[6,  4000] loss: 2.032
[6,  4200] loss: 2.032
[6,  4400] loss: 2.037
[6,  4600] loss: 2.039
[6,  4800] loss: 2.037
[6,  5000] loss: 2.046
Epoch [6/20], Train Loss: 0.2551, Val Loss: 0.2982, Test Loss: 2.3838Training Acc: 0.4168, Val Acc: 0.0604, Test Acc: 0.0622, 
[7,   200] loss: 2.029
[7,   400] loss: 2.037
[7,   600] loss: 2.023
[7,   800] loss: 2.028
[7,  1000] loss: 2.036
[7,  1200] loss: 2.035
[7,  1400] loss: 2.031
[7,  1600] loss: 2.026
[7,  1800] loss: 2.025
[7,  2000] loss: 2.023
[7,  2200] loss: 2.036
[7,  2400] loss: 2.031
[7,  2600] loss: 2.016
[7,  2800] loss: 2.032
[7,  3000] loss: 2.035
[7,  3200] loss: 2.015
[7,  3400] loss: 2.027
[7,  3600] loss: 2.030
[7,  3800] loss: 2.030
[7,  4000] loss: 2.014
[7,  4200] loss: 2.003
[7,  4400] loss: 2.019
[7,  4600] loss: 2.019
[7,  4800] loss: 2.005
[7,  5000] loss: 2.012
Epoch [7/20], Train Loss: 0.2531, Val Loss: 0.2907, Test Loss: 2.3268Training Acc: 0.4327, Val Acc: 0.1235, Test Acc: 0.1194, 
[8,   200] loss: 2.012
[8,   400] loss: 2.040
[8,   600] loss: 2.023
[8,   800] loss: 2.018
[8,  1000] loss: 2.028
[8,  1200] loss: 2.017
[8,  1400] loss: 2.017
[8,  1600] loss: 2.006
[8,  1800] loss: 2.005
[8,  2000] loss: 2.017
[8,  2200] loss: 2.009
[8,  2400] loss: 2.009
[8,  2600] loss: 2.006
[8,  2800] loss: 2.014
[8,  3000] loss: 2.016
[8,  3200] loss: 2.001
[8,  3400] loss: 2.022
[8,  3600] loss: 2.017
[8,  3800] loss: 1.999
[8,  4000] loss: 2.030
[8,  4200] loss: 2.006
[8,  4400] loss: 1.992
[8,  4600] loss: 1.997
[8,  4800] loss: 2.017
[8,  5000] loss: 2.002
Epoch [8/20], Train Loss: 0.2516, Val Loss: 0.2912, Test Loss: 2.3329Training Acc: 0.4449, Val Acc: 0.1229, Test Acc: 0.1170, 
[9,   200] loss: 1.990
[9,   400] loss: 2.010
[9,   600] loss: 2.019
[9,   800] loss: 2.006
[9,  1000] loss: 2.010
[9,  1200] loss: 1.995
[9,  1400] loss: 2.009
[9,  1600] loss: 1.986
[9,  1800] loss: 2.010
[9,  2000] loss: 2.003
[9,  2200] loss: 1.988
[9,  2400] loss: 1.992
[9,  2600] loss: 1.989
[9,  2800] loss: 2.004
[9,  3000] loss: 2.000
[9,  3200] loss: 2.001
[9,  3400] loss: 2.024
[9,  3600] loss: 2.000
[9,  3800] loss: 2.009
[9,  4000] loss: 2.000
[9,  4200] loss: 2.014
[9,  4400] loss: 2.010
[9,  4600] loss: 2.014
[9,  4800] loss: 2.000
[9,  5000] loss: 1.997
Epoch [9/20], Train Loss: 0.2504, Val Loss: 0.2918, Test Loss: 2.3325Training Acc: 0.4547, Val Acc: 0.1136, Test Acc: 0.1174, 
[10,   200] loss: 1.978
[10,   400] loss: 1.986
[10,   600] loss: 1.982
[10,   800] loss: 1.995
[10,  1000] loss: 2.008
[10,  1200] loss: 1.992
[10,  1400] loss: 1.991
[10,  1600] loss: 1.976
[10,  1800] loss: 2.017
[10,  2000] loss: 2.013
[10,  2200] loss: 1.994
[10,  2400] loss: 1.982
[10,  2600] loss: 2.009
[10,  2800] loss: 2.005
[10,  3000] loss: 1.990
[10,  3200] loss: 2.011
[10,  3400] loss: 2.017
[10,  3600] loss: 2.017
[10,  3800] loss: 1.982
[10,  4000] loss: 1.993
[10,  4200] loss: 1.998
[10,  4400] loss: 1.972
[10,  4600] loss: 1.994
[10,  4800] loss: 1.999
[10,  5000] loss: 1.993
Epoch [10/20], Train Loss: 0.2495, Val Loss: 0.2917, Test Loss: 2.3251Training Acc: 0.4614, Val Acc: 0.1165, Test Acc: 0.1247, 
[11,   200] loss: 1.981
[11,   400] loss: 1.987
[11,   600] loss: 1.997
[11,   800] loss: 1.985
[11,  1000] loss: 1.990
[11,  1200] loss: 2.005
[11,  1400] loss: 2.005
[11,  1600] loss: 1.992
[11,  1800] loss: 1.972
[11,  2000] loss: 1.964
[11,  2200] loss: 1.983
[11,  2400] loss: 1.974
[11,  2600] loss: 1.993
[11,  2800] loss: 1.991
[11,  3000] loss: 1.999
[11,  3200] loss: 2.000
[11,  3400] loss: 1.994
[11,  3600] loss: 1.991
[11,  3800] loss: 1.963
[11,  4000] loss: 1.974
[11,  4200] loss: 1.976
[11,  4400] loss: 1.974
[11,  4600] loss: 1.990
[11,  4800] loss: 2.000
[11,  5000] loss: 1.980
Epoch [11/20], Train Loss: 0.2483, Val Loss: 0.2923, Test Loss: 2.3319Training Acc: 0.4711, Val Acc: 0.1118, Test Acc: 0.1199, 
[12,   200] loss: 1.989
[12,   400] loss: 1.983
[12,   600] loss: 1.965
[12,   800] loss: 1.991
[12,  1000] loss: 1.994
[12,  1200] loss: 1.974
[12,  1400] loss: 1.999
[12,  1600] loss: 1.984
[12,  1800] loss: 1.990
[12,  2000] loss: 1.976
[12,  2200] loss: 1.976
[12,  2400] loss: 1.975
[12,  2600] loss: 1.985
[12,  2800] loss: 1.997
[12,  3000] loss: 1.979
[12,  3200] loss: 1.985
[12,  3400] loss: 1.971
[12,  3600] loss: 1.994
[12,  3800] loss: 1.981
[12,  4000] loss: 1.972
[12,  4200] loss: 1.963
[12,  4400] loss: 1.977
[12,  4600] loss: 1.970
[12,  4800] loss: 1.978
[12,  5000] loss: 1.986
Epoch [12/20], Train Loss: 0.2477, Val Loss: 0.2937, Test Loss: 2.3465Training Acc: 0.4769, Val Acc: 0.1010, Test Acc: 0.1048, 
[13,   200] loss: 1.973
[13,   400] loss: 1.973
[13,   600] loss: 1.974
[13,   800] loss: 1.964
[13,  1000] loss: 1.952
[13,  1200] loss: 1.985
[13,  1400] loss: 1.956
[13,  1600] loss: 1.980
[13,  1800] loss: 1.953
[13,  2000] loss: 2.001
[13,  2200] loss: 1.970
[13,  2400] loss: 1.999
[13,  2600] loss: 1.989
[13,  2800] loss: 1.987
[13,  3000] loss: 1.981
[13,  3200] loss: 1.954
[13,  3400] loss: 1.981
[13,  3600] loss: 1.959
[13,  3800] loss: 1.990
[13,  4000] loss: 1.965
[13,  4200] loss: 1.964
[13,  4400] loss: 1.968
[13,  4600] loss: 1.963
[13,  4800] loss: 1.971
[13,  5000] loss: 1.978
Epoch [13/20], Train Loss: 0.2466, Val Loss: 0.2950, Test Loss: 2.3600Training Acc: 0.4856, Val Acc: 0.0899, Test Acc: 0.0896, 
[14,   200] loss: 1.959
[14,   400] loss: 1.973
[14,   600] loss: 1.967
[14,   800] loss: 1.971
[14,  1000] loss: 1.970
[14,  1200] loss: 1.963
[14,  1400] loss: 1.955
[14,  1600] loss: 1.976
[14,  1800] loss: 1.963
[14,  2000] loss: 1.968
[14,  2200] loss: 1.968
[14,  2400] loss: 1.983
[14,  2600] loss: 1.991
[14,  2800] loss: 1.964
[14,  3000] loss: 1.949
[14,  3200] loss: 1.981
[14,  3400] loss: 1.989
[14,  3600] loss: 1.986
[14,  3800] loss: 1.974
[14,  4000] loss: 1.972
[14,  4200] loss: 1.952
[14,  4400] loss: 1.972
[14,  4600] loss: 1.970
[14,  4800] loss: 1.952
[14,  5000] loss: 1.990
Epoch [14/20], Train Loss: 0.2463, Val Loss: 0.2966, Test Loss: 2.3713Training Acc: 0.4868, Val Acc: 0.0778, Test Acc: 0.0784, 
[15,   200] loss: 1.952
[15,   400] loss: 1.963
[15,   600] loss: 1.983
[15,   800] loss: 1.973
[15,  1000] loss: 1.954
[15,  1200] loss: 1.974
[15,  1400] loss: 1.965
[15,  1600] loss: 1.963
[15,  1800] loss: 1.966
[15,  2000] loss: 1.956
[15,  2200] loss: 1.971
[15,  2400] loss: 1.939
[15,  2600] loss: 1.962
[15,  2800] loss: 1.966
[15,  3000] loss: 1.986
[15,  3200] loss: 1.986
[15,  3400] loss: 1.952
[15,  3600] loss: 1.941
[15,  3800] loss: 1.954
[15,  4000] loss: 1.975
[15,  4200] loss: 1.959
[15,  4400] loss: 1.953
[15,  4600] loss: 1.968
[15,  4800] loss: 1.978
[15,  5000] loss: 1.955
Epoch [15/20], Train Loss: 0.2455, Val Loss: 0.2967, Test Loss: 2.3763Training Acc: 0.4949, Val Acc: 0.0777, Test Acc: 0.0742, 
[16,   200] loss: 1.941
[16,   400] loss: 1.966
[16,   600] loss: 1.969
[16,   800] loss: 1.964
[16,  1000] loss: 1.962
[16,  1200] loss: 1.961
[16,  1400] loss: 1.941
[16,  1600] loss: 1.949
[16,  1800] loss: 1.970
[16,  2000] loss: 1.945
[16,  2200] loss: 1.943
[16,  2400] loss: 1.922
[16,  2600] loss: 1.942
[16,  2800] loss: 1.958
[16,  3000] loss: 1.980
[16,  3200] loss: 1.948
[16,  3400] loss: 1.953
[16,  3600] loss: 1.966
[16,  3800] loss: 1.954
[16,  4000] loss: 1.966
[16,  4200] loss: 1.960
[16,  4400] loss: 1.981
[16,  4600] loss: 1.956
[16,  4800] loss: 1.959
[16,  5000] loss: 1.966
Epoch [16/20], Train Loss: 0.2446, Val Loss: 0.2906, Test Loss: 2.3258Training Acc: 0.5008, Val Acc: 0.1278, Test Acc: 0.1274, 
[17,   200] loss: 1.960
[17,   400] loss: 1.945
[17,   600] loss: 1.963
[17,   800] loss: 1.971
[17,  1000] loss: 1.926
[17,  1200] loss: 1.958
[17,  1400] loss: 1.919
[17,  1600] loss: 1.967
[17,  1800] loss: 1.947
[17,  2000] loss: 1.945
[17,  2200] loss: 1.948
[17,  2400] loss: 1.951
[17,  2600] loss: 1.947
[17,  2800] loss: 1.939
[17,  3000] loss: 1.954
[17,  3200] loss: 1.975
[17,  3400] loss: 1.940
[17,  3600] loss: 1.974
[17,  3800] loss: 1.956
[17,  4000] loss: 1.954
[17,  4200] loss: 1.955
[17,  4400] loss: 1.934
[17,  4600] loss: 1.963
[17,  4800] loss: 1.955
[17,  5000] loss: 1.936
Epoch [17/20], Train Loss: 0.2439, Val Loss: 0.2920, Test Loss: 2.3360Training Acc: 0.5070, Val Acc: 0.1163, Test Acc: 0.1163, 
[18,   200] loss: 1.946
[18,   400] loss: 1.966
[18,   600] loss: 1.954
[18,   800] loss: 1.938
[18,  1000] loss: 1.943
[18,  1200] loss: 1.951
[18,  1400] loss: 1.957
[18,  1600] loss: 1.949
[18,  1800] loss: 1.947
[18,  2000] loss: 1.953
[18,  2200] loss: 1.947
[18,  2400] loss: 1.923
[18,  2600] loss: 1.954
[18,  2800] loss: 1.950
[18,  3000] loss: 1.969
[18,  3200] loss: 1.968
[18,  3400] loss: 1.970
[18,  3600] loss: 1.937
[18,  3800] loss: 1.961
[18,  4000] loss: 1.945
[18,  4200] loss: 1.954
[18,  4400] loss: 1.947
[18,  4600] loss: 1.984
[18,  4800] loss: 1.962
[18,  5000] loss: 1.931
Epoch [18/20], Train Loss: 0.2440, Val Loss: 0.2927, Test Loss: 2.3407Training Acc: 0.5049, Val Acc: 0.1094, Test Acc: 0.1114, 
[19,   200] loss: 1.950
[19,   400] loss: 1.937
[19,   600] loss: 1.951
[19,   800] loss: 1.924
[19,  1000] loss: 1.941
[19,  1200] loss: 1.947
[19,  1400] loss: 1.952
[19,  1600] loss: 1.940
[19,  1800] loss: 1.958
[19,  2000] loss: 1.943
[19,  2200] loss: 1.939
[19,  2400] loss: 1.951
[19,  2600] loss: 1.942
[19,  2800] loss: 1.945
[19,  3000] loss: 1.946
[19,  3200] loss: 1.958
[19,  3400] loss: 1.961
[19,  3600] loss: 1.944
[19,  3800] loss: 1.949
[19,  4000] loss: 1.957
[19,  4200] loss: 1.922
[19,  4400] loss: 1.955
[19,  4600] loss: 1.938
[19,  4800] loss: 1.962
[19,  5000] loss: 1.932
Epoch [19/20], Train Loss: 0.2432, Val Loss: 0.2935, Test Loss: 2.3474Training Acc: 0.5122, Val Acc: 0.1053, Test Acc: 0.1049, 
[20,   200] loss: 1.952
[20,   400] loss: 1.945
[20,   600] loss: 1.945
[20,   800] loss: 1.941
[20,  1000] loss: 1.923
[20,  1200] loss: 1.923
[20,  1400] loss: 1.926
[20,  1600] loss: 1.932
[20,  1800] loss: 1.936
[20,  2000] loss: 1.942
[20,  2200] loss: 1.942
[20,  2400] loss: 1.930
[20,  2600] loss: 1.964
[20,  2800] loss: 1.939
[20,  3000] loss: 1.936
[20,  3200] loss: 1.948
[20,  3400] loss: 1.921
[20,  3600] loss: 1.938
[20,  3800] loss: 1.950
[20,  4000] loss: 1.942
[20,  4200] loss: 1.965
[20,  4400] loss: 1.919
[20,  4600] loss: 1.917
[20,  4800] loss: 1.930
[20,  5000] loss: 1.958
Epoch [20/20], Train Loss: 0.2423, Val Loss: 0.2926, Test Loss: 2.3319Training Acc: 0.5191, Val Acc: 0.1116, Test Acc: 0.1200, 
Model saved as ../results/AlexNet/weights/model_final.pt

Training AlexNet version: AlexNet_No_L5
[1,   200] loss: 2.303
[1,   400] loss: 2.301
[1,   600] loss: 2.301
[1,   800] loss: 2.299
[1,  1000] loss: 2.298
[1,  1200] loss: 2.295
[1,  1400] loss: 2.292
[1,  1600] loss: 2.290
[1,  1800] loss: 2.284
[1,  2000] loss: 2.278
[1,  2200] loss: 2.273
[1,  2400] loss: 2.272
[1,  2600] loss: 2.262
[1,  2800] loss: 2.264
[1,  3000] loss: 2.256
[1,  3200] loss: 2.252
[1,  3400] loss: 2.242
[1,  3600] loss: 2.250
[1,  3800] loss: 2.236
[1,  4000] loss: 2.242
[1,  4200] loss: 2.233
[1,  4400] loss: 2.235
[1,  4600] loss: 2.233
[1,  4800] loss: 2.225
[1,  5000] loss: 2.215
Epoch [1/20], Train Loss: 0.2832, Val Loss: 0.2868, Test Loss: 2.2921Training Acc: 0.1757, Val Acc: 0.1113, Test Acc: 0.1120, 
[2,   200] loss: 2.214
[2,   400] loss: 2.216
[2,   600] loss: 2.216
[2,   800] loss: 2.207
[2,  1000] loss: 2.197
[2,  1200] loss: 2.203
[2,  1400] loss: 2.190
[2,  1600] loss: 2.195
[2,  1800] loss: 2.185
[2,  2000] loss: 2.181
[2,  2200] loss: 2.170
[2,  2400] loss: 2.161
[2,  2600] loss: 2.173
[2,  2800] loss: 2.154
[2,  3000] loss: 2.156
[2,  3200] loss: 2.148
[2,  3400] loss: 2.165
[2,  3600] loss: 2.133
[2,  3800] loss: 2.134
[2,  4000] loss: 2.131
[2,  4200] loss: 2.136
[2,  4400] loss: 2.123
[2,  4600] loss: 2.137
[2,  4800] loss: 2.124
[2,  5000] loss: 2.116
Epoch [2/20], Train Loss: 0.2708, Val Loss: 0.2918, Test Loss: 2.3379Training Acc: 0.3026, Val Acc: 0.0963, Test Acc: 0.0927, 
[3,   200] loss: 2.121
[3,   400] loss: 2.116
[3,   600] loss: 2.095
[3,   800] loss: 2.115
[3,  1000] loss: 2.093
[3,  1200] loss: 2.106
[3,  1400] loss: 2.090
[3,  1600] loss: 2.115
[3,  1800] loss: 2.094
[3,  2000] loss: 2.088
[3,  2200] loss: 2.099
[3,  2400] loss: 2.088
[3,  2600] loss: 2.092
[3,  2800] loss: 2.081
[3,  3000] loss: 2.086
[3,  3200] loss: 2.079
[3,  3400] loss: 2.077
[3,  3600] loss: 2.080
[3,  3800] loss: 2.091
[3,  4000] loss: 2.092
[3,  4200] loss: 2.086
[3,  4400] loss: 2.093
[3,  4600] loss: 2.098
[3,  4800] loss: 2.072
[3,  5000] loss: 2.054
Epoch [3/20], Train Loss: 0.2615, Val Loss: 0.2899, Test Loss: 2.3235Training Acc: 0.3743, Val Acc: 0.1234, Test Acc: 0.1165, 
[4,   200] loss: 2.076
[4,   400] loss: 2.092
[4,   600] loss: 2.076
[4,   800] loss: 2.061
[4,  1000] loss: 2.057
[4,  1200] loss: 2.068
[4,  1400] loss: 2.060
[4,  1600] loss: 2.043
[4,  1800] loss: 2.055
[4,  2000] loss: 2.067
[4,  2200] loss: 2.058
[4,  2400] loss: 2.041
[4,  2600] loss: 2.054
[4,  2800] loss: 2.066
[4,  3000] loss: 2.044
[4,  3200] loss: 2.046
[4,  3400] loss: 2.074
[4,  3600] loss: 2.050
[4,  3800] loss: 2.067
[4,  4000] loss: 2.070
[4,  4200] loss: 2.042
[4,  4400] loss: 2.052
[4,  4600] loss: 2.037
[4,  4800] loss: 2.036
[4,  5000] loss: 2.042
Epoch [4/20], Train Loss: 0.2572, Val Loss: 0.2925, Test Loss: 2.3431Training Acc: 0.4037, Val Acc: 0.0970, Test Acc: 0.0938, 
[5,   200] loss: 2.032
[5,   400] loss: 2.048
[5,   600] loss: 2.066
[5,   800] loss: 2.052
[5,  1000] loss: 2.042
[5,  1200] loss: 2.037
[5,  1400] loss: 2.036
[5,  1600] loss: 2.043
[5,  1800] loss: 2.043
[5,  2000] loss: 2.030
[5,  2200] loss: 2.039
[5,  2400] loss: 2.035
[5,  2600] loss: 2.026
[5,  2800] loss: 2.035
[5,  3000] loss: 2.039
[5,  3200] loss: 2.032
[5,  3400] loss: 2.036
[5,  3600] loss: 2.034
[5,  3800] loss: 2.005
[5,  4000] loss: 2.030
[5,  4200] loss: 2.025
[5,  4400] loss: 2.023
[5,  4600] loss: 2.036
[5,  4800] loss: 2.033
[5,  5000] loss: 2.029
Epoch [5/20], Train Loss: 0.2544, Val Loss: 0.2949, Test Loss: 2.3569Training Acc: 0.4251, Val Acc: 0.0829, Test Acc: 0.0849, 
[6,   200] loss: 2.026
[6,   400] loss: 2.007
[6,   600] loss: 2.013
[6,   800] loss: 2.018
[6,  1000] loss: 2.005
[6,  1200] loss: 2.015
[6,  1400] loss: 2.009
[6,  1600] loss: 2.030
[6,  1800] loss: 2.013
[6,  2000] loss: 2.034
[6,  2200] loss: 2.009
[6,  2400] loss: 2.047
[6,  2600] loss: 2.039
[6,  2800] loss: 2.012
[6,  3000] loss: 2.014
[6,  3200] loss: 2.032
[6,  3400] loss: 2.026
[6,  3600] loss: 2.020
[6,  3800] loss: 2.016
[6,  4000] loss: 2.013
[6,  4200] loss: 2.018
[6,  4400] loss: 2.016
[6,  4600] loss: 2.006
[6,  4800] loss: 2.009
[6,  5000] loss: 2.002
Epoch [6/20], Train Loss: 0.2522, Val Loss: 0.2916, Test Loss: 2.3268Training Acc: 0.4418, Val Acc: 0.1096, Test Acc: 0.1153, 
[7,   200] loss: 2.022
[7,   400] loss: 2.018
[7,   600] loss: 2.007
[7,   800] loss: 1.992
[7,  1000] loss: 2.005
[7,  1200] loss: 1.976
[7,  1400] loss: 2.009
[7,  1600] loss: 2.014
[7,  1800] loss: 2.008
[7,  2000] loss: 2.008
[7,  2200] loss: 2.007
[7,  2400] loss: 2.017
[7,  2600] loss: 2.009
[7,  2800] loss: 1.992
[7,  3000] loss: 2.002
[7,  3200] loss: 2.014
[7,  3400] loss: 2.008
[7,  3600] loss: 1.988
[7,  3800] loss: 1.999
[7,  4000] loss: 2.004
[7,  4200] loss: 2.032
[7,  4400] loss: 1.983
[7,  4600] loss: 1.996
[7,  4800] loss: 2.021
[7,  5000] loss: 1.997
Epoch [7/20], Train Loss: 0.2506, Val Loss: 0.2976, Test Loss: 2.3812Training Acc: 0.4541, Val Acc: 0.0636, Test Acc: 0.0650, 
[8,   200] loss: 2.007
[8,   400] loss: 1.982
[8,   600] loss: 1.979
[8,   800] loss: 1.995
[8,  1000] loss: 1.998
[8,  1200] loss: 2.000
[8,  1400] loss: 2.000
[8,  1600] loss: 1.997
[8,  1800] loss: 1.964
[8,  2000] loss: 1.980
[8,  2200] loss: 1.997
[8,  2400] loss: 1.982
[8,  2600] loss: 1.995
[8,  2800] loss: 2.006
[8,  3000] loss: 1.990
[8,  3200] loss: 1.961
[8,  3400] loss: 1.990
[8,  3600] loss: 1.995
[8,  3800] loss: 1.994
[8,  4000] loss: 2.008
[8,  4200] loss: 1.993
[8,  4400] loss: 1.981
[8,  4600] loss: 1.996
[8,  4800] loss: 1.982
[8,  5000] loss: 2.003
Epoch [8/20], Train Loss: 0.2489, Val Loss: 0.2930, Test Loss: 2.3458Training Acc: 0.4686, Val Acc: 0.1031, Test Acc: 0.1026, 
[9,   200] loss: 1.998
[9,   400] loss: 1.985
[9,   600] loss: 1.978
[9,   800] loss: 1.981
[9,  1000] loss: 1.969
[9,  1200] loss: 1.982
[9,  1400] loss: 1.974
[9,  1600] loss: 1.988
[9,  1800] loss: 1.965
[9,  2000] loss: 1.970
[9,  2200] loss: 1.995
[9,  2400] loss: 1.985
[9,  2600] loss: 2.006
[9,  2800] loss: 1.985
[9,  3000] loss: 1.988
[9,  3200] loss: 2.010
[9,  3400] loss: 1.985
[9,  3600] loss: 1.977
[9,  3800] loss: 1.995
[9,  4000] loss: 1.974
[9,  4200] loss: 1.991
[9,  4400] loss: 1.968
[9,  4600] loss: 1.983
[9,  4800] loss: 1.988
[9,  5000] loss: 1.975
Epoch [9/20], Train Loss: 0.2480, Val Loss: 0.2922, Test Loss: 2.3418Training Acc: 0.4764, Val Acc: 0.1103, Test Acc: 0.1053, 
[10,   200] loss: 2.000
[10,   400] loss: 1.979
[10,   600] loss: 1.968
[10,   800] loss: 1.974
[10,  1000] loss: 1.969
[10,  1200] loss: 1.983
[10,  1400] loss: 1.988
[10,  1600] loss: 1.988
[10,  1800] loss: 1.976
[10,  2000] loss: 1.969
[10,  2200] loss: 1.977
[10,  2400] loss: 1.977
[10,  2600] loss: 1.993
[10,  2800] loss: 1.972
[10,  3000] loss: 1.971
[10,  3200] loss: 1.976
[10,  3400] loss: 1.977
[10,  3600] loss: 1.971
[10,  3800] loss: 1.957
[10,  4000] loss: 1.989
[10,  4200] loss: 1.983
[10,  4400] loss: 1.964
[10,  4600] loss: 1.958
[10,  4800] loss: 1.964
[10,  5000] loss: 1.942
Epoch [10/20], Train Loss: 0.2468, Val Loss: 0.2937, Test Loss: 2.3478Training Acc: 0.4859, Val Acc: 0.0982, Test Acc: 0.1006, 
[11,   200] loss: 1.967
[11,   400] loss: 1.971
[11,   600] loss: 1.966
[11,   800] loss: 1.959
[11,  1000] loss: 1.963
[11,  1200] loss: 1.998
[11,  1400] loss: 1.982
[11,  1600] loss: 1.973
[11,  1800] loss: 1.956
[11,  2000] loss: 1.964
[11,  2200] loss: 1.963
[11,  2400] loss: 1.968
[11,  2600] loss: 1.974
[11,  2800] loss: 1.965
[11,  3000] loss: 1.975
[11,  3200] loss: 1.972
[11,  3400] loss: 1.979
[11,  3600] loss: 1.972
[11,  3800] loss: 1.971
[11,  4000] loss: 1.980
[11,  4200] loss: 1.964
[11,  4400] loss: 1.953
[11,  4600] loss: 1.962
[11,  4800] loss: 1.965
[11,  5000] loss: 1.967
Epoch [11/20], Train Loss: 0.2461, Val Loss: 0.2910, Test Loss: 2.3365Training Acc: 0.4910, Val Acc: 0.1202, Test Acc: 0.1106, 
[12,   200] loss: 1.973
[12,   400] loss: 1.948
[12,   600] loss: 1.951
[12,   800] loss: 1.965
[12,  1000] loss: 1.966
[12,  1200] loss: 1.968
[12,  1400] loss: 1.949
[12,  1600] loss: 1.968
[12,  1800] loss: 1.962
[12,  2000] loss: 1.968
[12,  2200] loss: 1.964
[12,  2400] loss: 1.968
[12,  2600] loss: 1.933
[12,  2800] loss: 1.954
[12,  3000] loss: 1.946
[12,  3200] loss: 1.975
[12,  3400] loss: 1.970
[12,  3600] loss: 1.960
[12,  3800] loss: 1.977
[12,  4000] loss: 1.949
[12,  4200] loss: 1.967
[12,  4400] loss: 1.954
[12,  4600] loss: 1.957
[12,  4800] loss: 1.954
[12,  5000] loss: 1.962
Epoch [12/20], Train Loss: 0.2450, Val Loss: 0.2965, Test Loss: 2.3701Training Acc: 0.4996, Val Acc: 0.0782, Test Acc: 0.0810, 
[13,   200] loss: 1.968
[13,   400] loss: 1.958
[13,   600] loss: 1.952
[13,   800] loss: 1.968
[13,  1000] loss: 1.953
[13,  1200] loss: 1.941
[13,  1400] loss: 1.966
[13,  1600] loss: 1.951
[13,  1800] loss: 1.953
[13,  2000] loss: 1.944
[13,  2200] loss: 1.940
[13,  2400] loss: 1.951
[13,  2600] loss: 1.971
[13,  2800] loss: 1.947
[13,  3000] loss: 1.943
[13,  3200] loss: 1.940
[13,  3400] loss: 1.968
[13,  3600] loss: 1.952
[13,  3800] loss: 1.947
[13,  4000] loss: 1.939
[13,  4200] loss: 1.946
[13,  4400] loss: 1.958
[13,  4600] loss: 1.966
[13,  4800] loss: 1.970
[13,  5000] loss: 1.955
Epoch [13/20], Train Loss: 0.2442, Val Loss: 0.2924, Test Loss: 2.3359Training Acc: 0.5060, Val Acc: 0.1097, Test Acc: 0.1147, 
[14,   200] loss: 1.954
[14,   400] loss: 1.941
[14,   600] loss: 1.944
[14,   800] loss: 1.953
[14,  1000] loss: 1.939
[14,  1200] loss: 1.966
[14,  1400] loss: 1.941
[14,  1600] loss: 1.951
[14,  1800] loss: 1.949
[14,  2000] loss: 1.953
[14,  2200] loss: 1.941
[14,  2400] loss: 1.935
[14,  2600] loss: 1.942
[14,  2800] loss: 1.918
[14,  3000] loss: 1.919
[14,  3200] loss: 1.934
[14,  3400] loss: 1.938
[14,  3600] loss: 1.957
[14,  3800] loss: 1.963
[14,  4000] loss: 1.956
[14,  4200] loss: 1.964
[14,  4400] loss: 1.955
[14,  4600] loss: 1.945
[14,  4800] loss: 1.952
[14,  5000] loss: 1.954
Epoch [14/20], Train Loss: 0.2433, Val Loss: 0.2905, Test Loss: 2.3262Training Acc: 0.5127, Val Acc: 0.1264, Test Acc: 0.1225, 
[15,   200] loss: 1.939
[15,   400] loss: 1.939
[15,   600] loss: 1.942
[15,   800] loss: 1.940
[15,  1000] loss: 1.924
[15,  1200] loss: 1.944
[15,  1400] loss: 1.938
[15,  1600] loss: 1.929
[15,  1800] loss: 1.960
[15,  2000] loss: 1.937
[15,  2200] loss: 1.922
[15,  2400] loss: 1.952
[15,  2600] loss: 1.937
[15,  2800] loss: 1.947
[15,  3000] loss: 1.943
[15,  3200] loss: 1.936
[15,  3400] loss: 1.965
[15,  3600] loss: 1.934
[15,  3800] loss: 1.959
[15,  4000] loss: 1.933
[15,  4200] loss: 1.941
[15,  4400] loss: 1.939
[15,  4600] loss: 1.939
[15,  4800] loss: 1.948
[15,  5000] loss: 1.950
Epoch [15/20], Train Loss: 0.2427, Val Loss: 0.2952, Test Loss: 2.3571Training Acc: 0.5201, Val Acc: 0.0871, Test Acc: 0.0929, 
[16,   200] loss: 1.939
[16,   400] loss: 1.926
[16,   600] loss: 1.938
[16,   800] loss: 1.928
[16,  1000] loss: 1.939
[16,  1200] loss: 1.942
[16,  1400] loss: 1.942
[16,  1600] loss: 1.947
[16,  1800] loss: 1.932
[16,  2000] loss: 1.942
[16,  2200] loss: 1.949
[16,  2400] loss: 1.959
[16,  2600] loss: 1.921
[16,  2800] loss: 1.945
[16,  3000] loss: 1.925
[16,  3200] loss: 1.940
[16,  3400] loss: 1.928
[16,  3600] loss: 1.924
[16,  3800] loss: 1.933
[16,  4000] loss: 1.940
[16,  4200] loss: 1.943
[16,  4400] loss: 1.945
[16,  4600] loss: 1.934
[16,  4800] loss: 1.931
[16,  5000] loss: 1.931
Epoch [16/20], Train Loss: 0.2421, Val Loss: 0.2959, Test Loss: 2.3639Training Acc: 0.5230, Val Acc: 0.0818, Test Acc: 0.0858, 
[17,   200] loss: 1.940
[17,   400] loss: 1.932
[17,   600] loss: 1.926
[17,   800] loss: 1.955
[17,  1000] loss: 1.935
[17,  1200] loss: 1.913
[17,  1400] loss: 1.935
[17,  1600] loss: 1.926
[17,  1800] loss: 1.934
[17,  2000] loss: 1.916
[17,  2200] loss: 1.928
[17,  2400] loss: 1.928
[17,  2600] loss: 1.942
[17,  2800] loss: 1.928
[17,  3000] loss: 1.933
[17,  3200] loss: 1.950
[17,  3400] loss: 1.927
[17,  3600] loss: 1.926
[17,  3800] loss: 1.942
[17,  4000] loss: 1.931
[17,  4200] loss: 1.942
[17,  4400] loss: 1.912
[17,  4600] loss: 1.933
[17,  4800] loss: 1.954
[17,  5000] loss: 1.932
Epoch [17/20], Train Loss: 0.2416, Val Loss: 0.2911, Test Loss: 2.3323Training Acc: 0.5278, Val Acc: 0.1222, Test Acc: 0.1172, 
[18,   200] loss: 1.938
[18,   400] loss: 1.931
[18,   600] loss: 1.912
[18,   800] loss: 1.939
[18,  1000] loss: 1.928
[18,  1200] loss: 1.914
[18,  1400] loss: 1.930
[18,  1600] loss: 1.917
[18,  1800] loss: 1.919
[18,  2000] loss: 1.922
[18,  2200] loss: 1.906
[18,  2400] loss: 1.904
[18,  2600] loss: 1.940
[18,  2800] loss: 1.934
[18,  3000] loss: 1.917
[18,  3200] loss: 1.942
[18,  3400] loss: 1.940
[18,  3600] loss: 1.906
[18,  3800] loss: 1.929
[18,  4000] loss: 1.952
[18,  4200] loss: 1.939
[18,  4400] loss: 1.952
[18,  4600] loss: 1.933
[18,  4800] loss: 1.936
[18,  5000] loss: 1.932
Epoch [18/20], Train Loss: 0.2411, Val Loss: 0.2950, Test Loss: 2.3590Training Acc: 0.5311, Val Acc: 0.0911, Test Acc: 0.0913, 
[19,   200] loss: 1.911
[19,   400] loss: 1.913
[19,   600] loss: 1.915
[19,   800] loss: 1.911
[19,  1000] loss: 1.908
[19,  1200] loss: 1.905
[19,  1400] loss: 1.917
[19,  1600] loss: 1.925
[19,  1800] loss: 1.929
[19,  2000] loss: 1.924
[19,  2200] loss: 1.920
[19,  2400] loss: 1.929
[19,  2600] loss: 1.931
[19,  2800] loss: 1.938
[19,  3000] loss: 1.938
[19,  3200] loss: 1.930
[19,  3400] loss: 1.920
[19,  3600] loss: 1.922
[19,  3800] loss: 1.919
[19,  4000] loss: 1.928
[19,  4200] loss: 1.915
[19,  4400] loss: 1.917
[19,  4600] loss: 1.919
[19,  4800] loss: 1.957
[19,  5000] loss: 1.903
Epoch [19/20], Train Loss: 0.2402, Val Loss: 0.2915, Test Loss: 2.3354Training Acc: 0.5373, Val Acc: 0.1194, Test Acc: 0.1153, 
[20,   200] loss: 1.910
[20,   400] loss: 1.926
[20,   600] loss: 1.921
[20,   800] loss: 1.935
[20,  1000] loss: 1.920
[20,  1200] loss: 1.907
[20,  1400] loss: 1.917
[20,  1600] loss: 1.920
[20,  1800] loss: 1.914
[20,  2000] loss: 1.917
[20,  2200] loss: 1.923
[20,  2400] loss: 1.915
[20,  2600] loss: 1.925
[20,  2800] loss: 1.916
[20,  3000] loss: 1.888
[20,  3200] loss: 1.922
[20,  3400] loss: 1.937
[20,  3600] loss: 1.912
[20,  3800] loss: 1.915
[20,  4000] loss: 1.908
[20,  4200] loss: 1.925
[20,  4400] loss: 1.918
[20,  4600] loss: 1.917
[20,  4800] loss: 1.927
[20,  5000] loss: 1.909
Epoch [20/20], Train Loss: 0.2397, Val Loss: 0.2925, Test Loss: 2.3451Training Acc: 0.5421, Val Acc: 0.1110, Test Acc: 0.1069, 
Model saved as ../results/AlexNet_No_L5/weights/model_final.pt

Training AlexNet version: AlexNet_No_L4
[1,   200] loss: 2.302
[1,   400] loss: 2.300
[1,   600] loss: 2.301
[1,   800] loss: 2.297
[1,  1000] loss: 2.296
[1,  1200] loss: 2.296
[1,  1400] loss: 2.296
[1,  1600] loss: 2.297
[1,  1800] loss: 2.295
[1,  2000] loss: 2.291
[1,  2200] loss: 2.292
[1,  2400] loss: 2.282
[1,  2600] loss: 2.293
[1,  2800] loss: 2.287
[1,  3000] loss: 2.277
[1,  3200] loss: 2.280
[1,  3400] loss: 2.273
[1,  3600] loss: 2.265
[1,  3800] loss: 2.273
[1,  4000] loss: 2.260
[1,  4200] loss: 2.257
[1,  4400] loss: 2.255
[1,  4600] loss: 2.255
[1,  4800] loss: 2.255
[1,  5000] loss: 2.247
Epoch [1/20], Train Loss: 0.2851, Val Loss: 0.2875, Test Loss: 2.2996Training Acc: 0.1425, Val Acc: 0.1487, Test Acc: 0.1517, 
[2,   200] loss: 2.244
[2,   400] loss: 2.230
[2,   600] loss: 2.238
[2,   800] loss: 2.239
[2,  1000] loss: 2.229
[2,  1200] loss: 2.229
[2,  1400] loss: 2.231
[2,  1600] loss: 2.231
[2,  1800] loss: 2.217
[2,  2000] loss: 2.224
[2,  2200] loss: 2.215
[2,  2400] loss: 2.222
[2,  2600] loss: 2.210
[2,  2800] loss: 2.210
[2,  3000] loss: 2.216
[2,  3200] loss: 2.206
[2,  3400] loss: 2.207
[2,  3600] loss: 2.202
[2,  3800] loss: 2.187
[2,  4000] loss: 2.197
[2,  4200] loss: 2.200
[2,  4400] loss: 2.205
[2,  4600] loss: 2.178
[2,  4800] loss: 2.188
[2,  5000] loss: 2.186
Epoch [2/20], Train Loss: 0.2767, Val Loss: 0.2896, Test Loss: 2.3144Training Acc: 0.2358, Val Acc: 0.1179, Test Acc: 0.1210, 
[3,   200] loss: 2.189
[3,   400] loss: 2.190
[3,   600] loss: 2.182
[3,   800] loss: 2.175
[3,  1000] loss: 2.176
[3,  1200] loss: 2.167
[3,  1400] loss: 2.166
[3,  1600] loss: 2.164
[3,  1800] loss: 2.171
[3,  2000] loss: 2.160
[3,  2200] loss: 2.159
[3,  2400] loss: 2.154
[3,  2600] loss: 2.147
[3,  2800] loss: 2.159
[3,  3000] loss: 2.163
[3,  3200] loss: 2.151
[3,  3400] loss: 2.154
[3,  3600] loss: 2.154
[3,  3800] loss: 2.149
[3,  4000] loss: 2.142
[3,  4200] loss: 2.139
[3,  4400] loss: 2.140
[3,  4600] loss: 2.142
[3,  4800] loss: 2.143
[3,  5000] loss: 2.145
Epoch [3/20], Train Loss: 0.2699, Val Loss: 0.2921, Test Loss: 2.3367Training Acc: 0.2976, Val Acc: 0.0948, Test Acc: 0.0913, 
[4,   200] loss: 2.138
[4,   400] loss: 2.133
[4,   600] loss: 2.125
[4,   800] loss: 2.129
[4,  1000] loss: 2.112
[4,  1200] loss: 2.116
[4,  1400] loss: 2.101
[4,  1600] loss: 2.101
[4,  1800] loss: 2.112
[4,  2000] loss: 2.127
[4,  2200] loss: 2.126
[4,  2400] loss: 2.099
[4,  2600] loss: 2.115
[4,  2800] loss: 2.105
[4,  3000] loss: 2.109
[4,  3200] loss: 2.112
[4,  3400] loss: 2.097
[4,  3600] loss: 2.124
[4,  3800] loss: 2.105
[4,  4000] loss: 2.109
[4,  4200] loss: 2.099
[4,  4400] loss: 2.089
[4,  4600] loss: 2.102
[4,  4800] loss: 2.103
[4,  5000] loss: 2.098
Epoch [4/20], Train Loss: 0.2639, Val Loss: 0.2925, Test Loss: 2.3364Training Acc: 0.3483, Val Acc: 0.1013, Test Acc: 0.1071, 
[5,   200] loss: 2.080
[5,   400] loss: 2.100
[5,   600] loss: 2.089
[5,   800] loss: 2.106
[5,  1000] loss: 2.082
[5,  1200] loss: 2.082
[5,  1400] loss: 2.077
[5,  1600] loss: 2.088
[5,  1800] loss: 2.081
[5,  2000] loss: 2.089
[5,  2200] loss: 2.083
[5,  2400] loss: 2.069
[5,  2600] loss: 2.084
[5,  2800] loss: 2.089
[5,  3000] loss: 2.076
[5,  3200] loss: 2.074
[5,  3400] loss: 2.076
[5,  3600] loss: 2.060
[5,  3800] loss: 2.078
[5,  4000] loss: 2.074
[5,  4200] loss: 2.092
[5,  4400] loss: 2.072
[5,  4600] loss: 2.080
[5,  4800] loss: 2.065
[5,  5000] loss: 2.061
Epoch [5/20], Train Loss: 0.2600, Val Loss: 0.2916, Test Loss: 2.3315Training Acc: 0.3782, Val Acc: 0.1113, Test Acc: 0.1114, 
[6,   200] loss: 2.068
[6,   400] loss: 2.055
[6,   600] loss: 2.072
[6,   800] loss: 2.060
[6,  1000] loss: 2.069
[6,  1200] loss: 2.060
[6,  1400] loss: 2.068
[6,  1600] loss: 2.054
[6,  1800] loss: 2.070
[6,  2000] loss: 2.069
[6,  2200] loss: 2.059
[6,  2400] loss: 2.051
[6,  2600] loss: 2.047
[6,  2800] loss: 2.072
[6,  3000] loss: 2.066
[6,  3200] loss: 2.052
[6,  3400] loss: 2.050
[6,  3600] loss: 2.060
[6,  3800] loss: 2.075
[6,  4000] loss: 2.075
[6,  4200] loss: 2.052
[6,  4400] loss: 2.048
[6,  4600] loss: 2.071
[6,  4800] loss: 2.056
[6,  5000] loss: 2.056
Epoch [6/20], Train Loss: 0.2577, Val Loss: 0.2944, Test Loss: 2.3549Training Acc: 0.3956, Val Acc: 0.0875, Test Acc: 0.0881, 
[7,   200] loss: 2.055
[7,   400] loss: 2.064
[7,   600] loss: 2.053
[7,   800] loss: 2.042
[7,  1000] loss: 2.065
[7,  1200] loss: 2.041
[7,  1400] loss: 2.054
[7,  1600] loss: 2.035
[7,  1800] loss: 2.035
[7,  2000] loss: 2.064
[7,  2200] loss: 2.043
[7,  2400] loss: 2.036
[7,  2600] loss: 2.044
[7,  2800] loss: 2.049
[7,  3000] loss: 2.026
[7,  3200] loss: 2.032
[7,  3400] loss: 2.029
[7,  3600] loss: 2.019
[7,  3800] loss: 2.050
[7,  4000] loss: 2.024
[7,  4200] loss: 2.040
[7,  4400] loss: 2.054
[7,  4600] loss: 2.056
[7,  4800] loss: 2.028
[7,  5000] loss: 2.036
Epoch [7/20], Train Loss: 0.2554, Val Loss: 0.2917, Test Loss: 2.3344Training Acc: 0.4139, Val Acc: 0.1131, Test Acc: 0.1097, 
[8,   200] loss: 2.029
[8,   400] loss: 2.044
[8,   600] loss: 2.027
[8,   800] loss: 2.025
[8,  1000] loss: 2.030
[8,  1200] loss: 2.035
[8,  1400] loss: 2.039
[8,  1600] loss: 2.035
[8,  1800] loss: 2.026
[8,  2000] loss: 2.029
[8,  2200] loss: 2.017
[8,  2400] loss: 2.034
[8,  2600] loss: 2.027
[8,  2800] loss: 2.039
[8,  3000] loss: 2.041
[8,  3200] loss: 2.049
[8,  3400] loss: 2.042
[8,  3600] loss: 2.004
[8,  3800] loss: 2.014
[8,  4000] loss: 2.024
[8,  4200] loss: 2.015
[8,  4400] loss: 2.026
[8,  4600] loss: 2.037
[8,  4800] loss: 2.032
[8,  5000] loss: 2.011
Epoch [8/20], Train Loss: 0.2537, Val Loss: 0.2953, Test Loss: 2.3621Training Acc: 0.4279, Val Acc: 0.0825, Test Acc: 0.0854, 
[9,   200] loss: 2.020
[9,   400] loss: 2.030
[9,   600] loss: 2.027
[9,   800] loss: 2.011
[9,  1000] loss: 2.015
[9,  1200] loss: 2.025
[9,  1400] loss: 2.031
[9,  1600] loss: 1.997
[9,  1800] loss: 2.021
[9,  2000] loss: 2.021
[9,  2200] loss: 2.010
[9,  2400] loss: 1.991
[9,  2600] loss: 2.035
[9,  2800] loss: 2.029
[9,  3000] loss: 2.033
[9,  3200] loss: 2.014
[9,  3400] loss: 2.022
[9,  3600] loss: 2.026
[9,  3800] loss: 2.024
[9,  4000] loss: 1.990
[9,  4200] loss: 2.020
[9,  4400] loss: 2.032
[9,  4600] loss: 2.037
[9,  4800] loss: 2.019
[9,  5000] loss: 2.031
Epoch [9/20], Train Loss: 0.2526, Val Loss: 0.2945, Test Loss: 2.3551Training Acc: 0.4363, Val Acc: 0.0923, Test Acc: 0.0923, 
[10,   200] loss: 2.018
[10,   400] loss: 2.008
[10,   600] loss: 2.026
[10,   800] loss: 2.001
[10,  1000] loss: 2.028
[10,  1200] loss: 2.007
[10,  1400] loss: 2.015
[10,  1600] loss: 2.003
[10,  1800] loss: 2.004
[10,  2000] loss: 2.016
[10,  2200] loss: 1.998
[10,  2400] loss: 2.018
[10,  2600] loss: 2.018
[10,  2800] loss: 2.008
[10,  3000] loss: 2.008
[10,  3200] loss: 1.992
[10,  3400] loss: 2.006
[10,  3600] loss: 2.012
[10,  3800] loss: 2.027
[10,  4000] loss: 1.996
[10,  4200] loss: 2.004
[10,  4400] loss: 2.011
[10,  4600] loss: 1.996
[10,  4800] loss: 2.007
[10,  5000] loss: 2.001
Epoch [10/20], Train Loss: 0.2511, Val Loss: 0.2951, Test Loss: 2.3593Training Acc: 0.4490, Val Acc: 0.0882, Test Acc: 0.0891, 
[11,   200] loss: 2.017
[11,   400] loss: 2.006
[11,   600] loss: 2.008
[11,   800] loss: 2.012
[11,  1000] loss: 2.011
[11,  1200] loss: 1.993
[11,  1400] loss: 2.004
[11,  1600] loss: 2.026
[11,  1800] loss: 1.998
[11,  2000] loss: 1.986
[11,  2200] loss: 1.996
[11,  2400] loss: 1.989
[11,  2600] loss: 1.999
[11,  2800] loss: 2.007
[11,  3000] loss: 2.010
[11,  3200] loss: 2.005
[11,  3400] loss: 1.987
[11,  3600] loss: 2.011
[11,  3800] loss: 2.000
[11,  4000] loss: 2.001
[11,  4200] loss: 1.995
[11,  4400] loss: 1.967
[11,  4600] loss: 2.007
[11,  4800] loss: 1.999
[11,  5000] loss: 2.007
Epoch [11/20], Train Loss: 0.2502, Val Loss: 0.2931, Test Loss: 2.3454Training Acc: 0.4560, Val Acc: 0.1063, Test Acc: 0.1054, 
[12,   200] loss: 1.994
[12,   400] loss: 2.022
[12,   600] loss: 1.996
[12,   800] loss: 2.015
[12,  1000] loss: 1.985
[12,  1200] loss: 2.004
[12,  1400] loss: 1.997
[12,  1600] loss: 1.983
[12,  1800] loss: 1.991
[12,  2000] loss: 2.004
[12,  2200] loss: 2.002
[12,  2400] loss: 1.984
[12,  2600] loss: 2.010
[12,  2800] loss: 1.991
[12,  3000] loss: 1.997
[12,  3200] loss: 1.985
[12,  3400] loss: 1.977
[12,  3600] loss: 2.003
[12,  3800] loss: 2.007
[12,  4000] loss: 1.999
[12,  4200] loss: 1.999
[12,  4400] loss: 1.993
[12,  4600] loss: 1.994
[12,  4800] loss: 2.012
[12,  5000] loss: 1.996
Epoch [12/20], Train Loss: 0.2497, Val Loss: 0.2956, Test Loss: 2.3700Training Acc: 0.4592, Val Acc: 0.0866, Test Acc: 0.0800, 
[13,   200] loss: 2.003
[13,   400] loss: 2.000
[13,   600] loss: 2.007
[13,   800] loss: 1.990
[13,  1000] loss: 1.980
[13,  1200] loss: 1.983
[13,  1400] loss: 2.000
[13,  1600] loss: 1.992
[13,  1800] loss: 1.977
[13,  2000] loss: 1.987
[13,  2200] loss: 1.981
[13,  2400] loss: 1.985
[13,  2600] loss: 1.989
[13,  2800] loss: 1.998
[13,  3000] loss: 1.988
[13,  3200] loss: 1.995
[13,  3400] loss: 1.990
[13,  3600] loss: 1.988
[13,  3800] loss: 1.996
[13,  4000] loss: 1.994
[13,  4200] loss: 2.005
[13,  4400] loss: 1.983
[13,  4600] loss: 1.972
[13,  4800] loss: 1.973
[13,  5000] loss: 1.975
Epoch [13/20], Train Loss: 0.2487, Val Loss: 0.2912, Test Loss: 2.3314Training Acc: 0.4685, Val Acc: 0.1195, Test Acc: 0.1165, 
[14,   200] loss: 1.983
[14,   400] loss: 1.982
[14,   600] loss: 1.981
[14,   800] loss: 1.988
[14,  1000] loss: 1.962
[14,  1200] loss: 1.991
[14,  1400] loss: 2.002
[14,  1600] loss: 1.977
[14,  1800] loss: 1.979
[14,  2000] loss: 1.981
[14,  2200] loss: 1.986
[14,  2400] loss: 1.978
[14,  2600] loss: 1.973
[14,  2800] loss: 1.987
[14,  3000] loss: 2.003
[14,  3200] loss: 1.976
[14,  3400] loss: 1.967
[14,  3600] loss: 2.013
[14,  3800] loss: 1.981
[14,  4000] loss: 1.996
[14,  4200] loss: 1.960
[14,  4400] loss: 1.981
[14,  4600] loss: 1.966
[14,  4800] loss: 1.992
[14,  5000] loss: 1.980
Epoch [14/20], Train Loss: 0.2478, Val Loss: 0.2928, Test Loss: 2.3406Training Acc: 0.4753, Val Acc: 0.1067, Test Acc: 0.1083, 
[15,   200] loss: 1.966
[15,   400] loss: 1.974
[15,   600] loss: 1.994
[15,   800] loss: 1.972
[15,  1000] loss: 1.992
[15,  1200] loss: 1.981
[15,  1400] loss: 1.972
[15,  1600] loss: 1.976
[15,  1800] loss: 1.966
[15,  2000] loss: 1.968
[15,  2200] loss: 1.973
[15,  2400] loss: 1.970
[15,  2600] loss: 1.974
[15,  2800] loss: 1.978
[15,  3000] loss: 1.970
[15,  3200] loss: 1.989
[15,  3400] loss: 1.956
[15,  3600] loss: 1.973
[15,  3800] loss: 1.991
[15,  4000] loss: 1.967
[15,  4200] loss: 1.986
[15,  4400] loss: 1.958
[15,  4600] loss: 1.995
[15,  4800] loss: 1.981
[15,  5000] loss: 1.967
Epoch [15/20], Train Loss: 0.2469, Val Loss: 0.2922, Test Loss: 2.3468Training Acc: 0.4839, Val Acc: 0.1133, Test Acc: 0.1021, 
[16,   200] loss: 1.959
[16,   400] loss: 1.973
[16,   600] loss: 2.000
[16,   800] loss: 1.966
[16,  1000] loss: 1.986
[16,  1200] loss: 1.976
[16,  1400] loss: 1.982
[16,  1600] loss: 1.958
[16,  1800] loss: 1.959
[16,  2000] loss: 1.962
[16,  2200] loss: 1.969
[16,  2400] loss: 1.986
[16,  2600] loss: 1.966
[16,  2800] loss: 1.954
[16,  3000] loss: 1.974
[16,  3200] loss: 1.972
[16,  3400] loss: 1.970
[16,  3600] loss: 1.979
[16,  3800] loss: 1.979
[16,  4000] loss: 1.971
[16,  4200] loss: 1.978
[16,  4400] loss: 1.980
[16,  4600] loss: 1.975
[16,  4800] loss: 1.955
[16,  5000] loss: 1.966
Epoch [16/20], Train Loss: 0.2465, Val Loss: 0.2982, Test Loss: 2.3885Training Acc: 0.4847, Val Acc: 0.0647, Test Acc: 0.0610, 
[17,   200] loss: 1.971
[17,   400] loss: 1.951
[17,   600] loss: 1.963
[17,   800] loss: 1.974
[17,  1000] loss: 1.975
[17,  1200] loss: 1.955
[17,  1400] loss: 1.948
[17,  1600] loss: 1.958
[17,  1800] loss: 1.953
[17,  2000] loss: 1.959
[17,  2200] loss: 1.961
[17,  2400] loss: 1.953
[17,  2600] loss: 1.979
[17,  2800] loss: 1.979
[17,  3000] loss: 1.982
[17,  3200] loss: 1.993
[17,  3400] loss: 1.982
[17,  3600] loss: 1.969
[17,  3800] loss: 1.962
[17,  4000] loss: 1.990
[17,  4200] loss: 1.976
[17,  4400] loss: 1.953
[17,  4600] loss: 1.951
[17,  4800] loss: 1.963
[17,  5000] loss: 1.972
Epoch [17/20], Train Loss: 0.2459, Val Loss: 0.2910, Test Loss: 2.3261Training Acc: 0.4902, Val Acc: 0.1232, Test Acc: 0.1241, 
[18,   200] loss: 1.976
[18,   400] loss: 1.959
[18,   600] loss: 1.967
[18,   800] loss: 1.975
[18,  1000] loss: 1.973
[18,  1200] loss: 1.960
[18,  1400] loss: 1.947
[18,  1600] loss: 1.974
[18,  1800] loss: 1.969
[18,  2000] loss: 1.977
[18,  2200] loss: 1.963
[18,  2400] loss: 1.982
[18,  2600] loss: 1.950
[18,  2800] loss: 1.963
[18,  3000] loss: 1.951
[18,  3200] loss: 1.966
[18,  3400] loss: 1.949
[18,  3600] loss: 1.998
[18,  3800] loss: 1.962
[18,  4000] loss: 1.950
[18,  4200] loss: 1.938
[18,  4400] loss: 1.957
[18,  4600] loss: 1.958
[18,  4800] loss: 1.949
[18,  5000] loss: 1.965
Epoch [18/20], Train Loss: 0.2454, Val Loss: 0.2968, Test Loss: 2.3796Training Acc: 0.4952, Val Acc: 0.0756, Test Acc: 0.0697, 
[19,   200] loss: 1.955
[19,   400] loss: 1.956
[19,   600] loss: 1.960
[19,   800] loss: 1.972
[19,  1000] loss: 1.954
[19,  1200] loss: 1.948
[19,  1400] loss: 1.952
[19,  1600] loss: 1.937
[19,  1800] loss: 1.961
[19,  2000] loss: 1.958
[19,  2200] loss: 1.963
[19,  2400] loss: 1.947
[19,  2600] loss: 1.961
[19,  2800] loss: 1.975
[19,  3000] loss: 1.940
[19,  3200] loss: 1.928
[19,  3400] loss: 1.963
[19,  3600] loss: 1.939
[19,  3800] loss: 1.957
[19,  4000] loss: 1.947
[19,  4200] loss: 1.944
[19,  4400] loss: 1.964
[19,  4600] loss: 1.976
[19,  4800] loss: 1.967
[19,  5000] loss: 1.950
Epoch [19/20], Train Loss: 0.2444, Val Loss: 0.2943, Test Loss: 2.3548Training Acc: 0.5029, Val Acc: 0.0961, Test Acc: 0.0981, 
[20,   200] loss: 1.964
[20,   400] loss: 1.965
[20,   600] loss: 1.946
[20,   800] loss: 1.965
[20,  1000] loss: 1.944
[20,  1200] loss: 1.943
[20,  1400] loss: 1.959
[20,  1600] loss: 1.955
[20,  1800] loss: 1.956
[20,  2000] loss: 1.940
[20,  2200] loss: 1.946
[20,  2400] loss: 1.960
[20,  2600] loss: 1.949
[20,  2800] loss: 1.955
[20,  3000] loss: 1.933
[20,  3200] loss: 1.960
[20,  3400] loss: 1.959
[20,  3600] loss: 1.973
[20,  3800] loss: 1.931
[20,  4000] loss: 1.953
[20,  4200] loss: 1.970
[20,  4400] loss: 1.931
[20,  4600] loss: 1.950
[20,  4800] loss: 1.963
[20,  5000] loss: 1.943
Epoch [20/20], Train Loss: 0.2440, Val Loss: 0.2937, Test Loss: 2.3499Training Acc: 0.5054, Val Acc: 0.1015, Test Acc: 0.1023, 
Model saved as ../results/AlexNet_No_L4/weights/model_final.pt

Training AlexNet version: AlexNet_No_FC1
[1,   200] loss: 2.300
[1,   400] loss: 2.295
[1,   600] loss: 2.286
[1,   800] loss: 2.278
[1,  1000] loss: 2.264
[1,  1200] loss: 2.256
[1,  1400] loss: 2.246
[1,  1600] loss: 2.244
[1,  1800] loss: 2.231
[1,  2000] loss: 2.223
[1,  2200] loss: 2.218
[1,  2400] loss: 2.193
[1,  2600] loss: 2.194
[1,  2800] loss: 2.183
[1,  3000] loss: 2.174
[1,  3200] loss: 2.181
[1,  3400] loss: 2.162
[1,  3600] loss: 2.155
[1,  3800] loss: 2.161
[1,  4000] loss: 2.151
[1,  4200] loss: 2.147
[1,  4400] loss: 2.140
[1,  4600] loss: 2.132
[1,  4800] loss: 2.143
[1,  5000] loss: 2.135
Epoch [1/20], Train Loss: 0.2755, Val Loss: 0.2875, Test Loss: 2.2971Training Acc: 0.2486, Val Acc: 0.1352, Test Acc: 0.1399, 
[2,   200] loss: 2.115
[2,   400] loss: 2.144
[2,   600] loss: 2.103
[2,   800] loss: 2.127
[2,  1000] loss: 2.116
[2,  1200] loss: 2.116
[2,  1400] loss: 2.109
[2,  1600] loss: 2.097
[2,  1800] loss: 2.107
[2,  2000] loss: 2.093
[2,  2200] loss: 2.109
[2,  2400] loss: 2.104
[2,  2600] loss: 2.090
[2,  2800] loss: 2.065
[2,  3000] loss: 2.080
[2,  3200] loss: 2.074
[2,  3400] loss: 2.076
[2,  3600] loss: 2.101
[2,  3800] loss: 2.092
[2,  4000] loss: 2.081
[2,  4200] loss: 2.089
[2,  4400] loss: 2.083
[2,  4600] loss: 2.061
[2,  4800] loss: 2.093
[2,  5000] loss: 2.087
Epoch [2/20], Train Loss: 0.2621, Val Loss: 0.2948, Test Loss: 2.3568Training Acc: 0.3635, Val Acc: 0.0830, Test Acc: 0.0829, 
[3,   200] loss: 2.068
[3,   400] loss: 2.071
[3,   600] loss: 2.072
[3,   800] loss: 2.071
[3,  1000] loss: 2.059
[3,  1200] loss: 2.054
[3,  1400] loss: 2.072
[3,  1600] loss: 2.079
[3,  1800] loss: 2.067
[3,  2000] loss: 2.062
[3,  2200] loss: 2.043
[3,  2400] loss: 2.049
[3,  2600] loss: 2.043
[3,  2800] loss: 2.056
[3,  3000] loss: 2.070
[3,  3200] loss: 2.068
[3,  3400] loss: 2.053
[3,  3600] loss: 2.045
[3,  3800] loss: 2.029
[3,  4000] loss: 2.063
[3,  4200] loss: 2.054
[3,  4400] loss: 2.046
[3,  4600] loss: 2.053
[3,  4800] loss: 2.051
[3,  5000] loss: 2.029
Epoch [3/20], Train Loss: 0.2571, Val Loss: 0.2895, Test Loss: 2.3164Training Acc: 0.4020, Val Acc: 0.1310, Test Acc: 0.1320, 
[4,   200] loss: 2.051
[4,   400] loss: 2.047
[4,   600] loss: 2.062
[4,   800] loss: 2.029
[4,  1000] loss: 2.026
[4,  1200] loss: 2.037
[4,  1400] loss: 2.038
[4,  1600] loss: 2.022
[4,  1800] loss: 2.036
[4,  2000] loss: 2.036
[4,  2200] loss: 2.044
[4,  2400] loss: 2.040
[4,  2600] loss: 2.013
[4,  2800] loss: 2.038
[4,  3000] loss: 2.029
[4,  3200] loss: 2.030
[4,  3400] loss: 2.025
[4,  3600] loss: 2.035
[4,  3800] loss: 2.040
[4,  4000] loss: 2.051
[4,  4200] loss: 2.013
[4,  4400] loss: 2.025
[4,  4600] loss: 2.018
[4,  4800] loss: 2.028
[4,  5000] loss: 2.017
Epoch [4/20], Train Loss: 0.2542, Val Loss: 0.2928, Test Loss: 2.3396Training Acc: 0.4244, Val Acc: 0.1021, Test Acc: 0.1066, 
[5,   200] loss: 2.016
[5,   400] loss: 2.013
[5,   600] loss: 2.036
[5,   800] loss: 2.033
[5,  1000] loss: 2.022
[5,  1200] loss: 2.015
[5,  1400] loss: 2.016
[5,  1600] loss: 2.020
[5,  1800] loss: 2.006
[5,  2000] loss: 2.022
[5,  2200] loss: 2.005
[5,  2400] loss: 2.021
[5,  2600] loss: 2.008
[5,  2800] loss: 2.012
[5,  3000] loss: 2.019
[5,  3200] loss: 2.018
[5,  3400] loss: 1.998
[5,  3600] loss: 2.011
[5,  3800] loss: 2.012
[5,  4000] loss: 2.020
[5,  4200] loss: 2.031
[5,  4400] loss: 1.992
[5,  4600] loss: 1.995
[5,  4800] loss: 2.006
[5,  5000] loss: 2.014
Epoch [5/20], Train Loss: 0.2518, Val Loss: 0.2977, Test Loss: 2.3825Training Acc: 0.4431, Val Acc: 0.0624, Test Acc: 0.0616, 
[6,   200] loss: 2.027
[6,   400] loss: 2.011
[6,   600] loss: 2.016
[6,   800] loss: 2.005
[6,  1000] loss: 2.017
[6,  1200] loss: 2.017
[6,  1400] loss: 1.998
[6,  1600] loss: 2.006
[6,  1800] loss: 2.011
[6,  2000] loss: 1.999
[6,  2200] loss: 2.001
[6,  2400] loss: 1.999
[6,  2600] loss: 2.012
[6,  2800] loss: 1.984
[6,  3000] loss: 2.016
[6,  3200] loss: 2.005
[6,  3400] loss: 1.986
[6,  3600] loss: 2.035
[6,  3800] loss: 1.994
[6,  4000] loss: 1.993
[6,  4200] loss: 1.993
[6,  4400] loss: 2.035
[6,  4600] loss: 1.992
[6,  4800] loss: 1.984
[6,  5000] loss: 1.996
Epoch [6/20], Train Loss: 0.2507, Val Loss: 0.2961, Test Loss: 2.3645Training Acc: 0.4515, Val Acc: 0.0795, Test Acc: 0.0828, 
[7,   200] loss: 1.989
[7,   400] loss: 1.990
[7,   600] loss: 2.001
[7,   800] loss: 2.014
[7,  1000] loss: 1.988
[7,  1200] loss: 1.999
[7,  1400] loss: 2.012
[7,  1600] loss: 1.985
[7,  1800] loss: 1.981
[7,  2000] loss: 2.003
[7,  2200] loss: 1.990
[7,  2400] loss: 2.004
[7,  2600] loss: 1.984
[7,  2800] loss: 2.002
[7,  3000] loss: 2.002
[7,  3200] loss: 1.990
[7,  3400] loss: 1.999
[7,  3600] loss: 2.001
[7,  3800] loss: 1.984
[7,  4000] loss: 1.975
[7,  4200] loss: 2.007
[7,  4400] loss: 2.002
[7,  4600] loss: 2.003
[7,  4800] loss: 2.000
[7,  5000] loss: 1.953
Epoch [7/20], Train Loss: 0.2493, Val Loss: 0.2925, Test Loss: 2.3409Training Acc: 0.4631, Val Acc: 0.1074, Test Acc: 0.1075, 
[8,   200] loss: 1.972
[8,   400] loss: 1.976
[8,   600] loss: 1.988
[8,   800] loss: 1.978
[8,  1000] loss: 1.982
[8,  1200] loss: 1.977
[8,  1400] loss: 1.999
[8,  1600] loss: 1.989
[8,  1800] loss: 2.009
[8,  2000] loss: 1.951
[8,  2200] loss: 1.997
[8,  2400] loss: 1.982
[8,  2600] loss: 1.981
[8,  2800] loss: 1.981
[8,  3000] loss: 1.999
[8,  3200] loss: 1.980
[8,  3400] loss: 1.980
[8,  3600] loss: 2.002
[8,  3800] loss: 1.985
[8,  4000] loss: 1.983
[8,  4200] loss: 1.978
[8,  4400] loss: 1.971
[8,  4600] loss: 1.983
[8,  4800] loss: 1.962
[8,  5000] loss: 1.981
Epoch [8/20], Train Loss: 0.2478, Val Loss: 0.2893, Test Loss: 2.3124Training Acc: 0.4749, Val Acc: 0.1378, Test Acc: 0.1400, 
[9,   200] loss: 1.971
[9,   400] loss: 1.987
[9,   600] loss: 1.951
[9,   800] loss: 1.983
[9,  1000] loss: 1.989
[9,  1200] loss: 1.982
[9,  1400] loss: 1.980
[9,  1600] loss: 1.983
[9,  1800] loss: 1.984
[9,  2000] loss: 1.978
[9,  2200] loss: 1.975
[9,  2400] loss: 1.986
[9,  2600] loss: 1.967
[9,  2800] loss: 1.984
[9,  3000] loss: 1.978
[9,  3200] loss: 1.960
[9,  3400] loss: 1.975
[9,  3600] loss: 1.993
[9,  3800] loss: 1.985
[9,  4000] loss: 1.986
[9,  4200] loss: 1.968
[9,  4400] loss: 1.986
[9,  4600] loss: 1.973
[9,  4800] loss: 1.991
[9,  5000] loss: 1.972
Epoch [9/20], Train Loss: 0.2473, Val Loss: 0.2930, Test Loss: 2.3462Training Acc: 0.4778, Val Acc: 0.1063, Test Acc: 0.1047, 
[10,   200] loss: 1.946
[10,   400] loss: 1.965
[10,   600] loss: 1.974
[10,   800] loss: 1.997
[10,  1000] loss: 1.966
[10,  1200] loss: 1.981
[10,  1400] loss: 1.953
[10,  1600] loss: 1.969
[10,  1800] loss: 1.958
[10,  2000] loss: 1.964
[10,  2200] loss: 1.994
[10,  2400] loss: 1.964
[10,  2600] loss: 1.963
[10,  2800] loss: 1.950
[10,  3000] loss: 1.975
[10,  3200] loss: 1.968
[10,  3400] loss: 1.965
[10,  3600] loss: 1.972
[10,  3800] loss: 1.985
[10,  4000] loss: 1.953
[10,  4200] loss: 1.966
[10,  4400] loss: 1.974
[10,  4600] loss: 1.965
[10,  4800] loss: 1.986
[10,  5000] loss: 1.968
Epoch [10/20], Train Loss: 0.2461, Val Loss: 0.2923, Test Loss: 2.3433Training Acc: 0.4891, Val Acc: 0.1136, Test Acc: 0.1084, 
[11,   200] loss: 1.954
[11,   400] loss: 1.961
[11,   600] loss: 1.974
[11,   800] loss: 1.952
[11,  1000] loss: 1.942
[11,  1200] loss: 1.946
[11,  1400] loss: 1.954
[11,  1600] loss: 1.955
[11,  1800] loss: 1.980
[11,  2000] loss: 1.962
[11,  2200] loss: 1.960
[11,  2400] loss: 1.994
[11,  2600] loss: 1.966
[11,  2800] loss: 1.959
[11,  3000] loss: 1.991
[11,  3200] loss: 1.960
[11,  3400] loss: 1.954
[11,  3600] loss: 1.975
[11,  3800] loss: 1.974
[11,  4000] loss: 1.969
[11,  4200] loss: 1.960
[11,  4400] loss: 1.951
[11,  4600] loss: 1.983
[11,  4800] loss: 1.985
[11,  5000] loss: 1.943
Epoch [11/20], Train Loss: 0.2455, Val Loss: 0.2944, Test Loss: 2.3529Training Acc: 0.4929, Val Acc: 0.0952, Test Acc: 0.0985, 
[12,   200] loss: 1.957
[12,   400] loss: 1.945
[12,   600] loss: 1.940
[12,   800] loss: 1.974
[12,  1000] loss: 1.985
[12,  1200] loss: 1.955
[12,  1400] loss: 1.956
[12,  1600] loss: 1.954
[12,  1800] loss: 1.967
[12,  2000] loss: 1.972
[12,  2200] loss: 1.966
[12,  2400] loss: 1.967
[12,  2600] loss: 1.944
[12,  2800] loss: 1.964
[12,  3000] loss: 1.955
[12,  3200] loss: 1.952
[12,  3400] loss: 1.960
[12,  3600] loss: 1.944
[12,  3800] loss: 1.978
[12,  4000] loss: 1.956
[12,  4200] loss: 1.948
[12,  4400] loss: 1.955
[12,  4600] loss: 1.963
[12,  4800] loss: 1.978
[12,  5000] loss: 1.963
Epoch [12/20], Train Loss: 0.2450, Val Loss: 0.3014, Test Loss: 2.4088Training Acc: 0.4974, Val Acc: 0.0407, Test Acc: 0.0426, 
[13,   200] loss: 1.958
[13,   400] loss: 1.960
[13,   600] loss: 1.942
[13,   800] loss: 1.943
[13,  1000] loss: 1.947
[13,  1200] loss: 1.960
[13,  1400] loss: 1.932
[13,  1600] loss: 1.973
[13,  1800] loss: 1.956
[13,  2000] loss: 1.963
[13,  2200] loss: 1.967
[13,  2400] loss: 1.954
[13,  2600] loss: 1.959
[13,  2800] loss: 1.952
[13,  3000] loss: 1.936
[13,  3200] loss: 1.968
[13,  3400] loss: 1.955
[13,  3600] loss: 1.962
[13,  3800] loss: 1.960
[13,  4000] loss: 1.964
[13,  4200] loss: 1.971
[13,  4400] loss: 1.925
[13,  4600] loss: 1.950
[13,  4800] loss: 1.963
[13,  5000] loss: 1.949
Epoch [13/20], Train Loss: 0.2443, Val Loss: 0.2942, Test Loss: 2.3528Training Acc: 0.5036, Val Acc: 0.1000, Test Acc: 0.1007, 
[14,   200] loss: 1.956
[14,   400] loss: 1.953
[14,   600] loss: 1.948
[14,   800] loss: 1.964
[14,  1000] loss: 1.950
[14,  1200] loss: 1.965
[14,  1400] loss: 1.959
[14,  1600] loss: 1.961
[14,  1800] loss: 1.938
[14,  2000] loss: 1.941
[14,  2200] loss: 1.951
[14,  2400] loss: 1.961
[14,  2600] loss: 1.965
[14,  2800] loss: 1.953
[14,  3000] loss: 1.968
[14,  3200] loss: 1.948
[14,  3400] loss: 1.951
[14,  3600] loss: 1.939
[14,  3800] loss: 1.928
[14,  4000] loss: 1.947
[14,  4200] loss: 1.942
[14,  4400] loss: 1.934
[14,  4600] loss: 1.926
[14,  4800] loss: 1.959
[14,  5000] loss: 1.965
Epoch [14/20], Train Loss: 0.2439, Val Loss: 0.2991, Test Loss: 2.3972Training Acc: 0.5060, Val Acc: 0.0582, Test Acc: 0.0553, 
[15,   200] loss: 1.938
[15,   400] loss: 1.936
[15,   600] loss: 1.944
[15,   800] loss: 1.954
[15,  1000] loss: 1.968
[15,  1200] loss: 1.935
[15,  1400] loss: 1.945
[15,  1600] loss: 1.941
[15,  1800] loss: 1.961
[15,  2000] loss: 1.943
[15,  2200] loss: 1.960
[15,  2400] loss: 1.929
[15,  2600] loss: 1.966
[15,  2800] loss: 1.934
[15,  3000] loss: 1.955
[15,  3200] loss: 1.939
[15,  3400] loss: 1.929
[15,  3600] loss: 1.926
[15,  3800] loss: 1.947
[15,  4000] loss: 1.936
[15,  4200] loss: 1.938
[15,  4400] loss: 1.958
[15,  4600] loss: 1.947
[15,  4800] loss: 1.949
[15,  5000] loss: 1.942
Epoch [15/20], Train Loss: 0.2431, Val Loss: 0.2934, Test Loss: 2.3480Training Acc: 0.5127, Val Acc: 0.1052, Test Acc: 0.1056, 
[16,   200] loss: 1.940
[16,   400] loss: 1.942
[16,   600] loss: 1.926
[16,   800] loss: 1.944
[16,  1000] loss: 1.940
[16,  1200] loss: 1.944
[16,  1400] loss: 1.962
[16,  1600] loss: 1.943
[16,  1800] loss: 1.947
[16,  2000] loss: 1.949
[16,  2200] loss: 1.953
[16,  2400] loss: 1.932
[16,  2600] loss: 1.937
[16,  2800] loss: 1.944
[16,  3000] loss: 1.943
[16,  3200] loss: 1.960
[16,  3400] loss: 1.947
[16,  3600] loss: 1.950
[16,  3800] loss: 1.951
[16,  4000] loss: 1.943
[16,  4200] loss: 1.933
[16,  4400] loss: 1.921
[16,  4600] loss: 1.947
[16,  4800] loss: 1.950
[16,  5000] loss: 1.939
Epoch [16/20], Train Loss: 0.2429, Val Loss: 0.2950, Test Loss: 2.3606Training Acc: 0.5141, Val Acc: 0.0931, Test Acc: 0.0921, 
[17,   200] loss: 1.938
[17,   400] loss: 1.940
[17,   600] loss: 1.966
[17,   800] loss: 1.927
[17,  1000] loss: 1.940
[17,  1200] loss: 1.938
[17,  1400] loss: 1.929
[17,  1600] loss: 1.934
[17,  1800] loss: 1.964
[17,  2000] loss: 1.927
[17,  2200] loss: 1.934
[17,  2400] loss: 1.926
[17,  2600] loss: 1.943
[17,  2800] loss: 1.935
[17,  3000] loss: 1.959
[17,  3200] loss: 1.920
[17,  3400] loss: 1.956
[17,  3600] loss: 1.920
[17,  3800] loss: 1.926
[17,  4000] loss: 1.938
[17,  4200] loss: 1.915
[17,  4400] loss: 1.944
[17,  4600] loss: 1.939
[17,  4800] loss: 1.925
[17,  5000] loss: 1.961
Epoch [17/20], Train Loss: 0.2422, Val Loss: 0.2915, Test Loss: 2.3398Training Acc: 0.5207, Val Acc: 0.1208, Test Acc: 0.1134, 
[18,   200] loss: 1.939
[18,   400] loss: 1.911
[18,   600] loss: 1.927
[18,   800] loss: 1.936
[18,  1000] loss: 1.931
[18,  1200] loss: 1.926
[18,  1400] loss: 1.959
[18,  1600] loss: 1.931
[18,  1800] loss: 1.941
[18,  2000] loss: 1.961
[18,  2200] loss: 1.965
[18,  2400] loss: 1.939
[18,  2600] loss: 1.948
[18,  2800] loss: 1.927
[18,  3000] loss: 1.930
[18,  3200] loss: 1.919
[18,  3400] loss: 1.947
[18,  3600] loss: 1.939
[18,  3800] loss: 1.933
[18,  4000] loss: 1.932
[18,  4200] loss: 1.916
[18,  4400] loss: 1.925
[18,  4600] loss: 1.939
[18,  4800] loss: 1.945
[18,  5000] loss: 1.939
Epoch [18/20], Train Loss: 0.2420, Val Loss: 0.2934, Test Loss: 2.3522Training Acc: 0.5213, Val Acc: 0.1063, Test Acc: 0.1021, 
[19,   200] loss: 1.921
[19,   400] loss: 1.944
[19,   600] loss: 1.910
[19,   800] loss: 1.937
[19,  1000] loss: 1.926
[19,  1200] loss: 1.932
[19,  1400] loss: 1.922
[19,  1600] loss: 1.925
[19,  1800] loss: 1.917
[19,  2000] loss: 1.905
[19,  2200] loss: 1.928
[19,  2400] loss: 1.924
[19,  2600] loss: 1.923
[19,  2800] loss: 1.932
[19,  3000] loss: 1.951
[19,  3200] loss: 1.955
[19,  3400] loss: 1.932
[19,  3600] loss: 1.928
[19,  3800] loss: 1.933
[19,  4000] loss: 1.913
[19,  4200] loss: 1.956
[19,  4400] loss: 1.946
[19,  4600] loss: 1.924
[19,  4800] loss: 1.939
[19,  5000] loss: 1.927
Epoch [19/20], Train Loss: 0.2412, Val Loss: 0.2975, Test Loss: 2.3791Training Acc: 0.5273, Val Acc: 0.0723, Test Acc: 0.0748, 
[20,   200] loss: 1.914
[20,   400] loss: 1.925
[20,   600] loss: 1.905
[20,   800] loss: 1.933
[20,  1000] loss: 1.932
[20,  1200] loss: 1.935
[20,  1400] loss: 1.930
[20,  1600] loss: 1.946
[20,  1800] loss: 1.926
[20,  2000] loss: 1.935
[20,  2200] loss: 1.917
[20,  2400] loss: 1.947
[20,  2600] loss: 1.921
[20,  2800] loss: 1.931
[20,  3000] loss: 1.937
[20,  3200] loss: 1.937
[20,  3400] loss: 1.927
[20,  3600] loss: 1.938
[20,  3800] loss: 1.944
[20,  4000] loss: 1.929
[20,  4200] loss: 1.935
[20,  4400] loss: 1.906
[20,  4600] loss: 1.946
[20,  4800] loss: 1.939
[20,  5000] loss: 1.929
Epoch [20/20], Train Loss: 0.2413, Val Loss: 0.2950, Test Loss: 2.3591Training Acc: 0.5279, Val Acc: 0.0945, Test Acc: 0.0947, 
Model saved as ../results/AlexNet_No_FC1/weights/model_final.pt
Evauating the models
Running inference on AlexNet
Accuracy: 5169/10000 (51.690000%)
Making the confusion matrix AlexNet
confusion matrix saved
Producing PR for AlexNet
Saving PR files AlexNet
Running inference on AlexNet_No_L5
Accuracy: 5336/10000 (53.360000%)
Making the confusion matrix AlexNet_No_L5
confusion matrix saved
Producing PR for AlexNet_No_L5
Saving PR files AlexNet_No_L5
Running inference on AlexNet_No_L4
Accuracy: 5094/10000 (50.940000%)
Making the confusion matrix AlexNet_No_L4
confusion matrix saved
Producing PR for AlexNet_No_L4
Saving PR files AlexNet_No_L4
Running inference on AlexNet_No_FC1
Accuracy: 5213/10000 (52.130000%)
Making the confusion matrix AlexNet_No_FC1
confusion matrix saved
Producing PR for AlexNet_No_FC1
Saving PR files AlexNet_No_FC1
